RESEARCH_TOPIC_DICT = {
    'vlm-attack': """The adversarial attack and defense of large language models: modern language models are RLHF-tuned so are aligned, but users can adversarially attack the model by injecting adversarially trained prompt inside to make models output unintentional content.""",
}